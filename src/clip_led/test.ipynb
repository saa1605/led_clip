{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import json \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import sys \n",
    "sys.path.append('../..')\n",
    "import src.utils as utils\n",
    "import src.clip as clip \n",
    "import yaml\n",
    "import math \n",
    "from tqdm import tqdm  \n",
    "# from src.clip_led.dataset import LEDDataset\n",
    "# from src.clip_led.model import LEDModel \n",
    "# from src.clip_led.engine import train_model, eval_model\n",
    "import src.fusion as fusion\n",
    "from src.blocks import Up, ConvBlock, IdentityBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import torch \n",
    "from torch.utils.data import Dataset \n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import numpy as np \n",
    "import src.clip as clip \n",
    "\n",
    "class LEDDataset(Dataset):\n",
    "    def __init__(self, data_path, image_dir, config):\n",
    "\n",
    "        # Gather train_data from {train/val/test}_data.json\n",
    "        self.data_path = data_path \n",
    "        self.data_file = open(self.data_path)\n",
    "        self.data = json.load(self.data_file)\n",
    "\n",
    "        # Extract the mode (train, valSeen, valUnseen) from the data_path \n",
    "        self.mode = self.data_path.split('/')[-1][:-5].split('_')[0]\n",
    "\n",
    "        # Store access to floorplans directory \n",
    "        self.image_dir = image_dir \n",
    "\n",
    "        # Save the global config \n",
    "        self.config = config \n",
    "\n",
    "        # Calculate parameters to adjust location based on scaling and cropping\n",
    "        self.crop_translate_x = (self.config['original_image_size'][2] - self.config['cropped_image_size'][2])/2 \n",
    "        self.crop_translate_y = (self.config['original_image_size'][1] - self.config['cropped_image_size'][1])/2 \n",
    "\n",
    "        self.resize_scale_x = self.config['scaled_image_size'][2] / self.config['cropped_image_size'][2]\n",
    "        self.resize_scale_y = self.config['scaled_image_size'][1] / self.config['cropped_image_size'][1]\n",
    "\n",
    "        # mesh2meters\n",
    "        self.mesh2meters_path = self.config['mesh2meters']\n",
    "        self.mesh2meters_file = open(self.mesh2meters_path)\n",
    "        self.mesh2meters = json.load(self.mesh2meters_file)\n",
    "\n",
    "        # transform required for CLIP \n",
    "        def convert_image_to_rgb(image):\n",
    "            return image.convert(\"RGB\")\n",
    "\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.CenterCrop((self.config['cropped_image_size'][1], self.config['cropped_image_size'][2])),\n",
    "            transforms.Resize(size=(self.config['scaled_image_size'][1], self.config['scaled_image_size'][2]), interpolation=transforms.InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "            convert_image_to_rgb,\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "        self.preprocess_visualize = transforms.Compose([\n",
    "            transforms.CenterCrop((self.config['cropped_image_size'][1], self.config['cropped_image_size'][2])),\n",
    "            transforms.Resize(size=(self.config['scaled_image_size'][1], self.config['scaled_image_size'][2]), interpolation=transforms.InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "            convert_image_to_rgb,\n",
    "        ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def gather_all_floors(self, index):\n",
    "        ''' Collect images for all floor for a given index\n",
    "            Ouptut: Maps(max_floors, (image_size)), Conversions(max_floors, 1)\n",
    "        '''\n",
    "\n",
    "        # Create empty tensors to hold maps and conversions \n",
    "        all_maps = torch.zeros(\n",
    "            self.config['max_floors'],\n",
    "            self.config[\"image_size\"][0],\n",
    "            self.config[\"image_size\"][1],\n",
    "            self.config[\"image_size\"][2],\n",
    "        )\n",
    "        all_conversions = torch.zeros(self.config[\"max_floors\"], 1)\n",
    "\n",
    "        # Extract scan_names and which floors that scan has from the data \n",
    "        scan_name = self.data[index]['scanName']\n",
    "        floors = self.mesh2meters[scan_name].keys()\n",
    "\n",
    "        # Iterate through each floor of a scan, open the image, preprocess it and convert it to a tensor \n",
    "        for enum, floor in enumerate(floors):\n",
    "            img = Image.open(f'{self.image_dir}floor_{floor}/{scan_name}_{floor}.png').convert('RGB')\n",
    "            if \"train\" in self.mode:\n",
    "                all_maps[enum, :, :, :] = self.preprocess(img)[:3, :, :]\n",
    "            else:\n",
    "                all_maps[enum, :, :, :] = self.preprocess(img)[:3, :, :]\n",
    "            all_conversions[enum, :] = self.mesh2meters[scan_name][floor][\"threeMeterRadius\"] / 3.0\n",
    "        return all_maps, all_conversions\n",
    "\n",
    "    def gather_correct_floor(self, index):\n",
    "        scan_name = self.data[index]['scanName']\n",
    "        x, y, floor = self.scale_location(index)\n",
    "        img = Image.open(f'{self.image_dir}floor_{floor}/{scan_name}_{floor}.png').convert('RGB')\n",
    "        \n",
    "        map = self.preprocess(img)\n",
    "        conversion = torch.tensor(self.mesh2meters[scan_name][str(floor)][\"threeMeterRadius\"] / 3.0).float()\n",
    "\n",
    "        return map, conversion\n",
    "\n",
    "    def scale_location(self, index):\n",
    "        if \"test\" in self.mode:\n",
    "            return [0, 0, 0]\n",
    "\n",
    "        floor = self.data[index]['finalLocation'][\"floor\"]\n",
    "        x, y = self.data[index]['finalLocation'][\"pixel_coord\"]    \n",
    "\n",
    "        return [int((x - self.crop_translate_x) * self.resize_scale_x), \n",
    "                int((y - self.crop_translate_y) * self.resize_scale_y), \n",
    "                floor] \n",
    "    \n",
    "\n",
    "    def create_target(self, index, x, y, floor):\n",
    "\n",
    "        scan_name = self.data[index]['scanName']\n",
    "        mesh_conversion =(self.mesh2meters[scan_name][str(floor)][\"threeMeterRadius\"] / 3.0)*(self.config['conversion_scale'])\n",
    "        gaussian_target = np.zeros(\n",
    "            (self.config['max_floors'], self.config['image_size'][1], self.config['image_size'][2])\n",
    "        )\n",
    "        gaussian_target[floor, y, x] = 1 # y, x because y -> rows and x -> columns\n",
    "        gaussian_target[floor, :, :] = gaussian_filter(\n",
    "            gaussian_target[floor, :, :],\n",
    "            sigma=(mesh_conversion),\n",
    "        )\n",
    "        gaussian_target[floor, :, :] = (\n",
    "            gaussian_target[floor, :, :]\n",
    "            / gaussian_target[floor, :, :].sum()\n",
    "        )\n",
    "        gaussian_target = torch.tensor(gaussian_target)\n",
    "        return gaussian_target\n",
    "        \n",
    "    def join_dialog(self, index):\n",
    "        dialogArray = self.data[index]['dialogArray']\n",
    "        return \" \".join(dialogArray)\n",
    "    \n",
    "    def visualize_target(self, index):\n",
    "        x, y, floor = self.scale_location(index)\n",
    "        \n",
    "        scan_name = self.data[index]['scanName']\n",
    "        img = Image.open(f'{self.image_dir}floor_{floor}/{scan_name}_{floor}.png').convert('RGB')\n",
    "\n",
    "        img_vis = self.preprocess_visualize(img)\n",
    "        draw = ImageDraw.Draw(img_vis)\n",
    "        draw.ellipse((x-10, y-10, x+10, y+10), 'red')\n",
    "        print(self.join_dialog(index))\n",
    "        img_vis.show()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target_x, target_y, target_floor = self.scale_location(index)\n",
    "        # maps, conversions = self.gather_correct_floor(index)\n",
    "        maps, conversions = self.gather_all_floors(index)\n",
    "        dialogs = clip.tokenize(self.join_dialog(index), truncate=True)\n",
    "        targets = self.create_target(index, target_x, target_y, target_floor)\n",
    "        scan_names = self.data[index]['scanName']\n",
    "        episode_ids = self.data[index]['episodeId']\n",
    "        true_viewpoints = self.data[index]['finalLocation']['viewPoint']\n",
    "\n",
    "        return {\n",
    "            'maps': maps,\n",
    "            'target_maps': targets, \n",
    "            'conversions': conversions,\n",
    "            'dialogs': dialogs,\n",
    "            'scan_names': scan_names, \n",
    "            'episode_ids': episode_ids,\n",
    "            'true_viewpoints': true_viewpoints,\n",
    "        }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch \n",
    "\n",
    "import src.clip as clip \n",
    "import src.fusion as fusion\n",
    "from src.blocks import Up, ConvBlock, IdentityBlock\n",
    "\n",
    "class LEDModel(nn.Module):\n",
    "    \"\"\" CLIP RN50 with U-Net skip connections \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(LEDModel, self).__init__()\n",
    "        self.config = config \n",
    "        self.up_factor = 2 if self.config['bilinear'] else 1\n",
    "        self.clip_rn50, self.preprocess = clip.load(\"RN50\")\n",
    "        self.clip_rn50 = self.clip_rn50.to(config['device'])\n",
    "\n",
    "        # Freezing the CLIP model\n",
    "        for param in self.clip_rn50.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self._build_decoder()\n",
    "\n",
    "\n",
    "    def _build_decoder(self):\n",
    "        # language\n",
    "        self.lang_fuser1 = fusion.names[self.config['lang_fusion_type']](input_dim=self.config['num_post_clip_channels'] // 2)\n",
    "        self.lang_fuser2 = fusion.names[self.config['lang_fusion_type']](input_dim=self.config['num_post_clip_channels'] // 4)\n",
    "        self.lang_fuser3 = fusion.names[self.config['lang_fusion_type']](input_dim=self.config['num_post_clip_channels'] // 8)\n",
    "\n",
    "        # CLIP encoder output -> 1024\n",
    "        self.proj_input_dim = 512 if 'word' in self.config['lang_fusion_type'] else 1024\n",
    "        self.lang_proj1 = nn.Linear(self.proj_input_dim, 1024)\n",
    "        self.lang_proj2 = nn.Linear(self.proj_input_dim, 512)\n",
    "        self.lang_proj3 = nn.Linear(self.proj_input_dim, 256)\n",
    "\n",
    "        # vision\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(self.config['num_post_clip_channels'], 1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.up1 = Up(2048, 1024 // self.up_factor, self.config['bilinear'])\n",
    "\n",
    "        self.up2 = Up(1024, 512 // self.up_factor, self.config['bilinear'])\n",
    "\n",
    "        self.up3 = Up(512, 256 // self.up_factor, self.config['bilinear'])\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ConvBlock(128, [64, 64, 64], kernel_size=3, stride=1, batchnorm=self.config['batch_norm']),\n",
    "            IdentityBlock(64, [64, 64, 64], kernel_size=3, stride=1, batchnorm=self.config['batch_norm']),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConvBlock(64, [32, 32, 32], kernel_size=3, stride=1, batchnorm=self.config['batch_norm']),\n",
    "            IdentityBlock(32, [32, 32, 32], kernel_size=3, stride=1, batchnorm=self.config['batch_norm']),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ConvBlock(32, [16, 16, 16], kernel_size=3, stride=1, batchnorm=self.config['batch_norm']),\n",
    "            IdentityBlock(16, [16, 16, 16], kernel_size=3, stride=1, batchnorm=self.config['batch_norm']),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=1),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, self.config['num_output_channels'], kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def encode_image(self, img):\n",
    "        with torch.no_grad():\n",
    "            # The default CLIP function has been updated to be able to get intermediate prepools \n",
    "            img_encoding, img_im = self.clip_rn50.visual.prepool_im(img)\n",
    "        return img_encoding, img_im\n",
    "\n",
    "    def encode_text(self, x):\n",
    "        with torch.no_grad():\n",
    "            text_feat = self.clip_rn50.encode_text(x)\n",
    "            text_feat = torch.repeat_interleave(text_feat, self.config['max_floors'], 0)\n",
    "        text_mask = torch.where(x.int()==0, x.int(), torch.tensor(1).int().cuda())  # [1, max_token_len]\n",
    "        return text_feat, text_mask\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, l):\n",
    "        B, num_maps, C, H, W = x.size()\n",
    "        x = x.view(B*num_maps, C, H, W)\n",
    "        in_type = x.dtype\n",
    "        in_shape = x.shape\n",
    "        x = x[:,:3]  # select RGB\n",
    "        x, im = self.encode_image(x)\n",
    "        x = x.to(in_type)\n",
    "\n",
    "        # encode text\n",
    "        l_enc, l_mask = self.encode_text(l)\n",
    "        l_input = l_enc\n",
    "        l_input = l_input.to(dtype=x.dtype)\n",
    "\n",
    "        # # encode image\n",
    "        assert x.shape[1] == self.config['num_post_clip_channels']\n",
    "        # print('after CLIP encoding: ', x.size())\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # print('after convolution after CLIP encoding: ', x.size())\n",
    "\n",
    "\n",
    "        x = self.lang_fuser1(x, l_input, x2_mask=l_mask, x2_proj=self.lang_proj1)\n",
    "        # print('after lang_fuser 1: ', x.size())\n",
    "        x = self.up1(x, im[-2])\n",
    "        # print('after up after lang_fuser 1: ', x.size())\n",
    "\n",
    "        x = self.lang_fuser2(x, l_input, x2_mask=l_mask, x2_proj=self.lang_proj2)\n",
    "        # print('after lang_fuser 2: ', x.size())\n",
    "        x = self.up2(x, im[-3])\n",
    "        # print('after up after lang_fuser 2: ', x.size())\n",
    "\n",
    "        x = self.lang_fuser3(x, l_input, x2_mask=l_mask, x2_proj=self.lang_proj3)\n",
    "        # print('after lang_fuser 3: ', x.size())\n",
    "        x = self.up3(x, im[-4])\n",
    "        # print('after up after lang_fuser 3: ', x.size())\n",
    "\n",
    "        for enum, layer in enumerate([self.layer1, self.layer2, self.layer3, self.conv2]):\n",
    "            x = layer(x)\n",
    "            # print(f'after layer {enum} after all lang_fusions', x.size())\n",
    "        \n",
    "        h, w = x.size()[-2], x.size()[-1]\n",
    "        x = x.squeeze(1)\n",
    "        x = x.view(B, num_maps, x.size()[-2], x.size()[-1])\n",
    "        x = F.log_softmax(x.view(B, -1), 1).view(B, num_maps, h, w)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "from src.utils import distance_from_pixels, accuracy\n",
    "import numpy as np \n",
    "\n",
    "def train_model(model, loader, loss_fn, optimizer, scaler, config):\n",
    "    acc5m = []\n",
    "    acc3m = []\n",
    "    acc0m = []\n",
    "    losses = []\n",
    "    localization_errors = []\n",
    "    for enum, data in enumerate(tqdm(loader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        maps = data['maps'].float().to(config['device'])\n",
    "        target_maps = data['target_maps'].float().to(config['device'])\n",
    "        conversions = data['conversions'].float()\n",
    "        dialogs = data['dialogs'].squeeze(1).to(config['device']) # The squeeze removes extra dimension in (BATCH_SIZE, 1, NUM_TOKENS)\n",
    "        # Data Required to calculate Localization Accuracy (Geodesic)\n",
    "        scan_names = data['scan_names']\n",
    "        true_viewpoints = data['true_viewpoints']\n",
    "        episode_ids = data['episode_ids']\n",
    "        with torch.autocast('cuda'):\n",
    "            preds = model(maps, dialogs)\n",
    "            loss = loss_fn(preds, target_maps)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scaler.update()\n",
    "\n",
    "        le, ep = distance_from_pixels(\n",
    "            config, preds.detach().cpu(), conversions, scan_names, true_viewpoints, episode_ids, 'train', )\n",
    "        losses.append(loss.item())\n",
    "        acc5m.append(accuracy(le, 5))\n",
    "        acc3m.append(accuracy(le, 3))\n",
    "        acc0m.append(accuracy(le, 0))\n",
    "        localization_errors.extend(le)\n",
    "    return {\n",
    "        'loss': np.mean(losses),\n",
    "        'acc5m': np.mean(np.asarray(acc5m)),\n",
    "        'acc3m': np.mean(np.asarray(acc3m)),\n",
    "        'acc0m': np.mean(np.asarray(acc0m)),\n",
    "    }\n",
    "\n",
    "def eval_model(model, loader, loss_fn, config, mode):\n",
    "    acc5m = []\n",
    "    acc3m = []\n",
    "    acc0m = []\n",
    "    losses = []\n",
    "    localization_errors = []\n",
    "    for enum, data in enumerate(tqdm(loader)):\n",
    "        maps = data['maps'].float().to(config['device'])\n",
    "        target_maps = data['target_maps'].float().to(config['device'])\n",
    "        conversions = data['conversions'].float()\n",
    "        dialogs = data['dialogs'].squeeze(1).to(config['device']) # The squeeze removes extra dimension in (BATCH_SIZE, 1, NUM_TOKENS)\n",
    "        # Data Required to calculate Localization Accuracy (Geodesic)\n",
    "        scan_names = data['scan_names']\n",
    "        true_viewpoints = data['true_viewpoints']\n",
    "        episode_ids = data['episode_ids']\n",
    "\n",
    "        # with torch.autocast('cpu'):\n",
    "        preds = model(maps, dialogs)\n",
    "        loss = loss_fn(preds, target_maps)\n",
    "\n",
    "\n",
    "        le, ep = distance_from_pixels(\n",
    "            config, preds.detach().cpu(), conversions, scan_names, true_viewpoints, episode_ids, mode)\n",
    "        losses.append(loss.item())\n",
    "        acc5m.append(accuracy(le, 5))\n",
    "        acc3m.append(accuracy(le, 3))\n",
    "        acc0m.append(accuracy(le, 0))\n",
    "        localization_errors.extend(le)\n",
    "    return {\n",
    "        'loss': np.mean(losses),\n",
    "        'acc5m': np.mean(np.asarray(acc5m)),\n",
    "        'acc3m': np.mean(np.asarray(acc3m)),\n",
    "        'acc0m': np.mean(np.asarray(acc0m)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to YAML\n",
    "config = {\n",
    "    # Data Paths\n",
    "    'train_path' : '../../data/way_splits/train_data.json',\n",
    "    'valid_seen_path' : '../../data/way_splits/valSeen_data.json',\n",
    "    'valid_unseen_path': '../../data/way_splits/valUnseen_data.json',\n",
    "    'mesh2meters': '../../data/floorplans/pix2meshDistance.json',\n",
    "    'image_dir': '../../data/floorplans/',\n",
    "    'geodistance_file': '../../data/geodistance_nodes.json',\n",
    "    'save_path': '../../logs/checkpoints',\n",
    "\n",
    "    'device': 'cuda:0',\n",
    "\n",
    "    # Hyper Parameters\n",
    "    'max_floors': 5,\n",
    "\n",
    "    # Image Parameters\n",
    "    'image_size': [3, 448, 448],\n",
    "    # 'image_size': [3, 700, 1200],\n",
    "    'original_image_size': [3, 700, 1200],\n",
    "    'cropped_image_size': [3, 700, 800],\n",
    "    'scaled_image_size': [3, 448, 448],\n",
    "\n",
    "\n",
    "    'crop_translate_x': 200,\n",
    "    'crop_translate_y': 0,\n",
    "    'resize_scale_x': 448/800,\n",
    "    'resize_scale_y': 448/700,\n",
    "    'conversion_scale': 448/800,\n",
    "\n",
    "\n",
    "    'lang_fusion_type': 'mult',\n",
    "    'num_post_clip_channels': 2048, \n",
    "    'bilinear': True,\n",
    "    'batch_norm': True, \n",
    "    'num_output_channels': 1,\n",
    "\n",
    "    'lr': 0.001,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop \n",
    "\n",
    "\n",
    "def training_loop(train_loader, valid_seen_loader, valid_unseen_loader, epochs, model, loss_fn, optimizer, scaler, scheduler, config):\n",
    "\n",
    "    # Metrics \n",
    "    metrics = {\n",
    "        'train_loss': 0,\n",
    "        'valid_seen_loss': 0,\n",
    "        'valid_unseen_loss': 0,\n",
    "        'train_acc_5m': 0, \n",
    "        'train_acc_3m': 0, \n",
    "        'train_acc_0m': 0, \n",
    "        'valid_seen_acc_5m': 0, \n",
    "        'valid_seen_acc_3m': 0, \n",
    "        'valid_seen_acc_0m': 0, \n",
    "        'valid_unseen_acc_5m': 0,\n",
    "        'valid_unsseen_acc_3m': 0,\n",
    "        'valid_unsseen_acc_0m': 0,\n",
    "    }\n",
    "    best_loss = float('inf')\n",
    "    # Training \n",
    "    for e in range(epochs): \n",
    "\n",
    "        model.train()\n",
    "        train_metrics = train_model(model, train_loader, loss_fn, optimizer, scaler, config)\n",
    "        \n",
    "        print(f'Train Loss: {train_metrics[\"loss\"]}')\n",
    "        print(f'Train Acc0m: {train_metrics[\"acc0m\"]}')\n",
    "        print(f'Train Acc3m: {train_metrics[\"acc3m\"]}')\n",
    "        print(f'Train Acc5m: {train_metrics[\"acc5m\"]}')\n",
    "        \n",
    "        utils.assign_metrics(metrics, train_metrics, 'train')\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        valid_seen_metrics = eval_model(model, valid_seen_loader, loss_fn, config, 'valid_seen')\n",
    "\n",
    "        print(f'Valid Seen Loss: {valid_seen_metrics[\"loss\"]}')\n",
    "        print(f'Valid Seen Acc0m: {valid_seen_metrics[\"acc0m\"]}')\n",
    "        print(f'Valid Seen Acc3m: {valid_seen_metrics[\"acc3m\"]}')\n",
    "        print(f'Valid Seen Acc5m: {valid_seen_metrics[\"acc5m\"]}')\n",
    "\n",
    "        utils.assign_metrics(metrics, valid_seen_metrics, 'valid_seen')\n",
    "\n",
    "        valid_unseen_metrics = eval_model(model, valid_seen_loader, loss_fn, config, 'valid_unseen')\n",
    "\n",
    "        print(f'Valid Unseen Loss: {valid_seen_metrics[\"loss\"]}')\n",
    "        print(f'Valid Unseen Acc0m: {valid_seen_metrics[\"acc0m\"]}')\n",
    "        print(f'Valid Unseen Acc3m: {valid_seen_metrics[\"acc3m\"]}')\n",
    "        print(f'Valid Unseen Acc5m: {valid_seen_metrics[\"acc5m\"]}')\n",
    "\n",
    "        utils.assign_metrics(metrics, valid_unseen_metrics, 'valid_unseen')\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "        if metrics['valid_unseen_loss'] < best_loss:\n",
    "            save_dict = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "            save_path = config['save_path'] + f'_epoch_{e}_loss_{metrics[\"valid_unseen_loss\"]}.pth'\n",
    "            torch.save(save_dict, save_path)\n",
    "            best_loss = metrics['valid_unseen_loss']\n",
    "        \n",
    "        scheduler.step(metrics['valid_unseen_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Datasets, Creating DataLoaders\n"
     ]
    }
   ],
   "source": [
    "train_dataset = LEDDataset(config['train_path'], config['image_dir'], config)\n",
    "valid_seen_dataset = LEDDataset(config['train_path'], config['image_dir'], config)\n",
    "valid_unseen_dataset = LEDDataset(config['train_path'], config['image_dir'], config)\n",
    "print(\"Created Datasets, Creating DataLoaders\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=3)\n",
    "valid_seen_loader = DataLoader(valid_seen_dataset, batch_size=6)\n",
    "valid_unseen_loader = DataLoader(valid_unseen_dataset, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataLoaders, Instantiating Model\n"
     ]
    }
   ],
   "source": [
    "print(\"Created DataLoaders, Instantiating Model\")\n",
    "led_clip = LEDModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "led_clip.to(config['device']); pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated Model, Configuring Training Parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"Instantiated Model, Configuring Training Parameters\")\n",
    "loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "optimizer = torch.optim.AdamW(led_clip.parameters(), lr=config['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1350 [00:00<?, ?it/s]/home/saaket/miniconda3/envs/graphled/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "  1%|          | 16/1350 [00:30<41:55,  1.89s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 490 is out of bounds for axis 2 with size 448",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6839/3054350058.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_seen_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_unseen_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mled_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6839/1643045151.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(train_loader, valid_seen_loader, valid_unseen_loader, epochs, model, loss_fn, optimizer, scaler, scheduler, config)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Train Loss: {train_metrics[\"loss\"]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6839/3064880830.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loader, loss_fn, optimizer, scaler, config)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlocalization_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0menum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/graphled/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/graphled/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/graphled/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/graphled/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/graphled/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6839/1760229361.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mmaps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_all_floors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mdialogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_dialog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_floor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mscan_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scanName'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mepisode_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'episodeId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6839/1760229361.py\u001b[0m in \u001b[0;36mcreate_target\u001b[0;34m(self, index, x, y, floor)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_floors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         )\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mgaussian_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# y, x because y -> rows and x -> columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         gaussian_target[floor, :, :] = gaussian_filter(\n\u001b[1;32m    117\u001b[0m             \u001b[0mgaussian_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 490 is out of bounds for axis 2 with size 448"
     ]
    }
   ],
   "source": [
    "training_loop(train_loader, valid_seen_loader, valid_unseen_loader, 10, led_clip, loss_fn, optimizer, scaler, scheduler, config) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"some weird text\", \"some more odd text\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type int but found long int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5121/2959010493.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# text_feat = torch.repeat_interleave(text_feat, self.num_maps, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtext_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [1, max_token_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type int but found long int"
     ]
    }
   ],
   "source": [
    "tokens = clip.tokenize(a, truncate=True).to('cuda:0')\n",
    "\n",
    "# text_feat = self.clip_rn50.encode_text(tokens)\n",
    "# text_feat = torch.repeat_interleave(text_feat, self.num_maps, 0)\n",
    "\n",
    "text_mask = torch.where(tokens==0, tokens, 1)  # [1, max_token_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1).int().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tok.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-16130,    836,   5613,   4160, -16129,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [-16130,    836,    750,  11387,   4160, -16129,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(a_tok==0, a_tok, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1915fb457f177848a436c53e0d85f261306c0429b7a27e35e26917a207fd56ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graphled')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
