{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/graphled/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import json \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import sys \n",
    "sys.path.append('../..')\n",
    "import src.utils as utils\n",
    "import src.clip as clip \n",
    "import yaml\n",
    "import math \n",
    "from tqdm import tqdm  \n",
    "from src.clip_led.dataset import LEDDataset\n",
    "\n",
    "import src.fusion as fusion\n",
    "from src.blocks import Up, ConvBlock, IdentityBlock\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Data Paths\n",
    "    'train_path' : '../../data/way_splits/train_data.json',\n",
    "    'valid_seen_path' : '../../data/way_splits/valSeen_data.json',\n",
    "    'valid_unseen_path': '../../data/way_splits/valUnseen_data.json',\n",
    "    'mesh2meters': '../../data/floorplans/pix2meshDistance.json',\n",
    "    'image_dir': '../../data/floorplans/',\n",
    "    'geodistance_file': '../../data/geodistance_nodes.json',\n",
    "\n",
    "    'device': 'cpu',\n",
    "\n",
    "    # Hyper Parameters\n",
    "    'max_floors': 5,\n",
    "\n",
    "    # Image Parameters\n",
    "    'image_size': [3, 448, 448],\n",
    "    # 'image_size': [3, 700, 1200],\n",
    "    'original_image_size': [3, 700, 1200],\n",
    "    'cropped_image_size': [3, 700, 800],\n",
    "    'scaled_image_size': [3, 448, 448],\n",
    "\n",
    "\n",
    "    'crop_translate_x': 200,\n",
    "    'crop_translate_y': 0,\n",
    "    'resize_scale_x': 448/800,\n",
    "    'resize_scale_y': 448/700,\n",
    "    'conversion_scale': 448/800,\n",
    "\n",
    "\n",
    "    'lang_fusion_type': 'mult',\n",
    "    'num_post_clip_channels': 2048, \n",
    "    'bilinear': True,\n",
    "    'batch_norm': True, \n",
    "    'num_output_channels': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.478775426276762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['dialogs'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LEDDataset(config['valid_seen_path'], config['image_dir'], config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.478775426276762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['maps', 'target_maps', 'conversions', 'dialogs', 'scan_names', 'episode_ids', 'true_viewpoints'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_rn50, preprocess = clip.load(\"RN50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEDModel(nn.Module):\n",
    "    \"\"\" CLIP RN50 with U-Net skip connections \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(LEDModel, self).__init__()\n",
    "        self.config = config \n",
    "        self.up_factor = 2 if self.config['bilinear'] else 1\n",
    "        self.clip_rn50, self.preprocess = clip.load(\"RN50\")\n",
    "\n",
    "        # Freezing the CLIP model\n",
    "        for param in self.clip_rn50.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self._build_decoder()\n",
    "\n",
    "\n",
    "    def _build_decoder(self):\n",
    "        # language\n",
    "        self.lang_fuser1 = fusion.names[self.config['lang_fusion_type']](input_dim=self.config['num_post_clip_channels'] // 2)\n",
    "        self.lang_fuser2 = fusion.names[self.config['lang_fusion_type']](input_dim=self.config['num_post_clip_channels'] // 4)\n",
    "        self.lang_fuser3 = fusion.names[self.config['lang_fusion_type']](input_dim=self.config['num_post_clip_channels'] // 8)\n",
    "\n",
    "        # CLIP encoder output -> 1024\n",
    "        self.proj_input_dim = 512 if 'word' in self.config['lang_fusion_type'] else 1024\n",
    "        self.lang_proj1 = nn.Linear(self.proj_input_dim, 1024)\n",
    "        self.lang_proj2 = nn.Linear(self.proj_input_dim, 512)\n",
    "        self.lang_proj3 = nn.Linear(self.proj_input_dim, 256)\n",
    "\n",
    "        # vision\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(self.config['num_post_clip_channels'], 1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.up1 = Up(2048, 1024 // self.up_factor, self.config['bilinear'])\n",
    "\n",
    "        self.up2 = Up(1024, 512 // self.up_factor, self.config['bilinear'])\n",
    "\n",
    "        self.up3 = Up(512, 256 // self.up_factor, self.config['bilinear'])\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ConvBlock(128, [64, 64, 64], kernel_size=3, stride=1, batchnorm=self.config['batch_norm']),\n",
    "            IdentityBlock(64, [64, 64, 64], kernel_size=3, stride=1, batchnorm=self.config['batch_norm']),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConvBlock(64, [32, 32, 32], kernel_size=3, stride=1, batchnorm=self.config['batch_norm']),\n",
    "            IdentityBlock(32, [32, 32, 32], kernel_size=3, stride=1, batchnorm=self.config['batch_norm']),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ConvBlock(32, [16, 16, 16], kernel_size=3, stride=1, batchnorm=self.config['batch_norm']),\n",
    "            IdentityBlock(16, [16, 16, 16], kernel_size=3, stride=1, batchnorm=self.config['batch_norm']),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=1),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, self.config['num_output_channels'], kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def encode_image(self, img):\n",
    "        with torch.no_grad():\n",
    "            # The default CLIP function has been updated to be able to get intermediate prepools \n",
    "            img_encoding, img_im = self.clip_rn50.visual.prepool_im(img)\n",
    "        return img_encoding, img_im\n",
    "\n",
    "    def encode_text(self, x):\n",
    "        x = x.type(torch.LongTensor)\n",
    "        with torch.no_grad():\n",
    "            text_feat = self.clip_rn50.encode_text(x)\n",
    "            text_feat = torch.repeat_interleave(text_feat, self.config['max_floors'], 0)\n",
    "\n",
    "        text_mask = torch.where(x==0, x, 1)  # [1, max_token_len]\n",
    "        return text_feat, text_mask\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, l):\n",
    "        B, num_maps, C, H, W = x.size()\n",
    "        x = x.view(B*num_maps, C, H, W)\n",
    "        in_type = x.dtype\n",
    "        in_shape = x.shape\n",
    "        x = x[:,:3]  # select RGB\n",
    "        x, im = self.encode_image(x)\n",
    "        x = x.to(in_type)\n",
    "\n",
    "        # encode text\n",
    "        l_enc, l_mask = self.encode_text(l)\n",
    "        l_input = l_enc\n",
    "        l_input = l_input.to(dtype=x.dtype)\n",
    "\n",
    "        # # encode image\n",
    "        assert x.shape[1] == self.config['num_post_clip_channels']\n",
    "        # print('after CLIP encoding: ', x.size())\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # print('after convolution after CLIP encoding: ', x.size())\n",
    "\n",
    "\n",
    "        x = self.lang_fuser1(x, l_input, x2_mask=l_mask, x2_proj=self.lang_proj1)\n",
    "        # print('after lang_fuser 1: ', x.size())\n",
    "        x = self.up1(x, im[-2])\n",
    "        # print('after up after lang_fuser 1: ', x.size())\n",
    "\n",
    "        x = self.lang_fuser2(x, l_input, x2_mask=l_mask, x2_proj=self.lang_proj2)\n",
    "        # print('after lang_fuser 2: ', x.size())\n",
    "        x = self.up2(x, im[-3])\n",
    "        # print('after up after lang_fuser 2: ', x.size())\n",
    "\n",
    "        x = self.lang_fuser3(x, l_input, x2_mask=l_mask, x2_proj=self.lang_proj3)\n",
    "        # print('after lang_fuser 3: ', x.size())\n",
    "        x = self.up3(x, im[-4])\n",
    "        # print('after up after lang_fuser 3: ', x.size())\n",
    "\n",
    "        for enum, layer in enumerate([self.layer1, self.layer2, self.layer3, self.conv2]):\n",
    "            x = layer(x)\n",
    "            # print(f'after layer {enum} after all lang_fusions', x.size())\n",
    "        \n",
    "        h, w = x.size()[-2], x.size()[-1]\n",
    "        x = x.squeeze(1)\n",
    "        x = x.view(B, num_maps, x.size()[-2], x.size()[-1])\n",
    "        x = F.log_softmax(x.view(B, -1), 1).view(B, num_maps, h, w)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "led_clip = LEDModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.478775426276762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 448, 448])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['maps'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.478775426276762\n",
      "11.478775426276762\n"
     ]
    }
   ],
   "source": [
    "preds = led_clip(train_dataset[0]['maps'].unsqueeze(0), train_dataset[0]['dialogs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.478775426276762\n",
      "11.478775426276762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3, 448, 448])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps, dialogs = train_dataset[0]['maps'], train_dataset[0]['dialogs']\n",
    "maps = maps.unsqueeze(0)\n",
    "maps.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 448, 448])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters \n",
    "\n",
    "loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "optimizer = torch.optim.AdamW(led_clip.parameters(), lr=config['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "scheduler = torch.optim.ReduceLROnPlateau(optimizer, 'min')\n",
    "scaler = torch.cuda.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def snap_to_grid(geodistance_nodes, node2pix, sn, pred_coord, conversion, level):\n",
    "    min_dist = math.inf\n",
    "    best_node = \"\"\n",
    "    for node in node2pix[sn].keys():\n",
    "        if node2pix[sn][node][2] != int(level) or node not in geodistance_nodes:\n",
    "            continue\n",
    "        target_coord = [node2pix[sn][node][0][1], node2pix[sn][node][0][0]]\n",
    "        dist = np.sqrt(\n",
    "            (target_coord[0] - pred_coord[0]) ** 2\n",
    "            + (target_coord[1] - pred_coord[1]) ** 2\n",
    "        ) / (conversion)\n",
    "        if dist.item() < min_dist:\n",
    "            best_node = node\n",
    "            min_dist = dist.item()\n",
    "    return best_node\n",
    "\n",
    "\n",
    "def distance_from_pixels(config, preds, mesh_conversions, scan_names, true_viewpoints, episode_ids, mode):\n",
    "    \"\"\"Calculate distances between model predictions and targets within a batch.\n",
    "    Takes the propablity map over the pixels and returns the geodesic distance\"\"\"\n",
    "    node2pix = json.load(open(config['image_dir'] + \"allScans_Node2pix.json\"))\n",
    "    geodistance_nodes = json.load(open(config['geodistance_file']))\n",
    "    distances, episode_predictions = [], []\n",
    "    print(scan_names)\n",
    "    for pred, conversion, sn, tv, id in zip(\n",
    "        preds, mesh_conversions, scan_names, true_viewpoints, episode_ids\n",
    "    ):\n",
    "\n",
    "        total_floors = len(set([v[2] for k, v in node2pix[sn].items()]))\n",
    "        pred = nn.functional.interpolate(\n",
    "            pred.unsqueeze(1), (700, 1200), mode=\"bilinear\"\n",
    "        ).squeeze(1)[:total_floors]\n",
    "        pred_coord = np.unravel_index(pred.argmax(), pred.size())\n",
    "        convers = conversion.view(config['max_floors'], 1, 1)[pred_coord[0].item()]\n",
    "        pred_viewpoint = snap_to_grid(\n",
    "            geodistance_nodes[sn],\n",
    "            node2pix,\n",
    "            sn,\n",
    "            [pred_coord[1].item(), pred_coord[2].item()],\n",
    "            convers,\n",
    "            pred_coord[0].item(),\n",
    "        )\n",
    "        if mode != \"test\":\n",
    "            dist = geodistance_nodes[sn][tv][pred_viewpoint]\n",
    "            distances.append(dist)\n",
    "        episode_predictions.append([id, pred_viewpoint])\n",
    "    return distances, episode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.478775426276762\n",
      "16.93066725972494\n",
      "torch.Size([2, 77])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    maps = data['maps']\n",
    "    target_maps = data['target_maps']\n",
    "    conversions = data['conversions']\n",
    "    dialogs = data['dialogs']\n",
    "    dialogs = dialogs.squeeze(1)\n",
    "    print(dialogs.size())\n",
    "\n",
    "    preds = led_clip(maps, dialogs)\n",
    "    break \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VLzqgDo317F', 'VVfe2KiqLaN']\n"
     ]
    }
   ],
   "source": [
    "a, b = distance_from_pixels(config, preds, data['conversions'], data['scan_names'], data['true_viewpoints'], data['episode_ids'], train_dataset.mode )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dists, threshold=3):\n",
    "    \"\"\"Calculating accuracy at 3 meters by default\"\"\"\n",
    "    return np.mean((torch.tensor(dists) <= threshold).int().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop \n",
    "\n",
    "\n",
    "def train(train_loader, valid_seen_loader, valid_unseen_loader, epochs, model, loss_fn, optimizer, scaler, scheduler):\n",
    "\n",
    "    # Training \n",
    "    for enum, data in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        maps = data['maps']\n",
    "        target_maps = data['target_maps']\n",
    "        conversions = data['conversions']\n",
    "        dialogs = data['dialogs']\n",
    "\n",
    "        with torch.autocast():\n",
    "            preds = model(maps, dialogs)\n",
    "            loss = loss_fn(preds, target_maps)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        scaler.update()\n",
    "\n",
    "        le, ep = distance_from_pixels(\n",
    "            args, preds.detach().cpu(), batch_conversions, info_elem, mode\n",
    "        )\n",
    "        return loss, accuracy(le, 0), accuracy(le, 5), ep\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1915fb457f177848a436c53e0d85f261306c0429b7a27e35e26917a207fd56ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graphled')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
