{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import json \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import sys \n",
    "sys.path.append('../..')\n",
    "import src.utils as utils\n",
    "import src.clip as clip \n",
    "import yaml \n",
    "from src.clip_led.dataset import LEDDataset\n",
    "\n",
    "import src.fusion as fusion\n",
    "from src.blocks import Up, ConvBlock, IdentityBlock\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Data Paths\n",
    "    'train_path' : '../../data/way_splits/train_data.json',\n",
    "    'valid_seen_path' : '../../data/way_splits/valSeen_data.json',\n",
    "    'valid_unseen_path': '../../data/way_splits/valUnseen_data.json',\n",
    "    'mesh2meters': '../../data/floorplans/pix2meshDistance.json',\n",
    "    'image_dir': '../../data/floorplans/',\n",
    "\n",
    "    'device': 'cpu',\n",
    "\n",
    "    # Hyper Parameters\n",
    "    'max_floors': 5,\n",
    "\n",
    "    # Image Parameters\n",
    "    'image_size': [3, 448, 448],\n",
    "    # 'image_size': [3, 700, 1200],\n",
    "    'original_image_size': [3, 700, 1200],\n",
    "    'cropped_image_size': [3, 700, 800],\n",
    "    'scaled_image_size': [3, 448, 448],\n",
    "\n",
    "\n",
    "    'crop_translate_x': 200,\n",
    "    'crop_translate_y': 0,\n",
    "    'resize_scale_x': 448/800,\n",
    "    'resize_scale_y': 448/700,\n",
    "    'conversion_scale': 448/800\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LEDDataset(config['valid_seen_path'], config['image_dir'], config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_rn50, preprocess = clip.load(\"RN50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEDModel(nn.Module):\n",
    "    \"\"\" CLIP RN50 with U-Net skip connections \"\"\"\n",
    "    def __init__(self, args):\n",
    "        super(LEDModel, self).__init__()\n",
    "        self.args = args \n",
    "        # self.output_dim = self.args.output_dim\n",
    "        self.num_maps = self.args.num_maps\n",
    "        self.output_dim = self.args.output_dim\n",
    "        self.input_dim = self.args.input_dim  # penultimate layer channel-size of CLIP-RN50\n",
    "        self.device = self.args.device \n",
    "        self.batchnorm = self.args.batchnorm\n",
    "        self.lang_fusion_type = self.args.lang_fusion_type\n",
    "        self.bilinear = self.args.bilinear\n",
    "        self.batch_size = self.args.batch_size\n",
    "        self.up_factor = 2 if self.bilinear else 1\n",
    "        self.clip_rn50, self.preprocess = clip.load(\"RN50\", device=self.args.device)\n",
    "        model, _ = clip.load_clip(\"RN50\", device=self.device)\n",
    "        self.clip_rn50 = clip.build_model(model.state_dict()).to(self.device)\n",
    "\n",
    "        self._build_decoder()\n",
    "\n",
    "\n",
    "    def _build_decoder(self):\n",
    "        # language\n",
    "        self.lang_fuser1 = fusion.names[self.lang_fusion_type](input_dim=self.input_dim // 2)\n",
    "        self.lang_fuser2 = fusion.names[self.lang_fusion_type](input_dim=self.input_dim // 4)\n",
    "        self.lang_fuser3 = fusion.names[self.lang_fusion_type](input_dim=self.input_dim // 8)\n",
    "\n",
    "        # CLIP encoder output -> 1024\n",
    "        self.proj_input_dim = 512 if 'word' in self.lang_fusion_type else 1024\n",
    "        self.lang_proj1 = nn.Linear(self.proj_input_dim, 1024)\n",
    "        self.lang_proj2 = nn.Linear(self.proj_input_dim, 512)\n",
    "        self.lang_proj3 = nn.Linear(self.proj_input_dim, 256)\n",
    "\n",
    "        # vision\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.up1 = Up(2048, 1024 // self.up_factor, self.bilinear)\n",
    "\n",
    "        self.up2 = Up(1024, 512 // self.up_factor, self.bilinear)\n",
    "\n",
    "        self.up3 = Up(512, 256 // self.up_factor, self.bilinear)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ConvBlock(128, [64, 64, 64], kernel_size=3, stride=1, batchnorm=self.batchnorm),\n",
    "            IdentityBlock(64, [64, 64, 64], kernel_size=3, stride=1, batchnorm=self.batchnorm),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConvBlock(64, [32, 32, 32], kernel_size=3, stride=1, batchnorm=self.batchnorm),\n",
    "            IdentityBlock(32, [32, 32, 32], kernel_size=3, stride=1, batchnorm=self.batchnorm),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ConvBlock(32, [16, 16, 16], kernel_size=3, stride=1, batchnorm=self.batchnorm),\n",
    "            IdentityBlock(16, [16, 16, 16], kernel_size=3, stride=1, batchnorm=self.batchnorm),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, self.output_dim, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def encode_image(self, img):\n",
    "        with torch.no_grad():\n",
    "            # The default CLIP function has been updated to be able to get intermediate prepools \n",
    "            img_encoding, img_im = self.clip_rn50.visual.prepool_im(img)\n",
    "        return img_encoding, img_im\n",
    "\n",
    "    def encode_text(self, x):\n",
    "        with torch.no_grad():\n",
    "            tokens = clip.tokenize(x, truncate=True).to(self.device)\n",
    "\n",
    "            text_feat = self.clip_rn50.encode_text(tokens)\n",
    "            text_feat = torch.repeat_interleave(text_feat, self.num_maps, 0)\n",
    "\n",
    "        text_mask = torch.where(tokens==0, tokens, 1)  # [1, max_token_len]\n",
    "        return text_feat, text_mask\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, l):\n",
    "        B, num_maps, C, H, W = x.size()\n",
    "        x = x.view(B*num_maps, C, H, W)\n",
    "        in_type = x.dtype\n",
    "        in_shape = x.shape\n",
    "        x = x[:,:3]  # select RGB\n",
    "        x, im = self.encode_image(x)\n",
    "        x = x.to(in_type)\n",
    "\n",
    "        # encode text\n",
    "        l_enc, l_mask = self.encode_text(l)\n",
    "        l_input = l_enc\n",
    "        l_input = l_input.to(dtype=x.dtype)\n",
    "\n",
    "        # # encode image\n",
    "        assert x.shape[1] == self.input_dim\n",
    "        x = self.conv1(x)\n",
    "\n",
    "\n",
    "\n",
    "        x = self.lang_fuser1(x, l_input, x2_mask=l_mask, x2_proj=self.lang_proj1)\n",
    "        x = self.up1(x, im[-2])\n",
    "\n",
    "        x = self.lang_fuser2(x, l_input, x2_mask=l_mask, x2_proj=self.lang_proj2)\n",
    "        x = self.up2(x, im[-3])\n",
    "\n",
    "        x = self.lang_fuser3(x, l_input, x2_mask=l_mask, x2_proj=self.lang_proj3)\n",
    "        x = self.up3(x, im[-4])\n",
    "\n",
    "        for layer in [self.layer1, self.layer2, self.layer3, self.conv2]:\n",
    "            x = layer(x)\n",
    "\n",
    "        # x = F.interpolate(x, size=(780, 455), mode='bilinear')\n",
    "        h, w = x.size()[-2], x.size()[-1]\n",
    "        x = x.squeeze(1)\n",
    "        x = x.view(B, num_maps, x.size()[-2], x.size()[-1])\n",
    "        x = F.log_softmax(x.view(B, -1), 1).view(B, num_maps, h, w)\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1915fb457f177848a436c53e0d85f261306c0429b7a27e35e26917a207fd56ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graphled')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
