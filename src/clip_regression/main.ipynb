{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import json \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image \n",
    "import clip \n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load('RN50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config = {\n",
    "    # Data Paths\n",
    "    'train_path' : '../../data/way_splits/train_data.json',\n",
    "    'valid_seen_path' : '../../data/way_splits/valSeen_data.json',\n",
    "    'valid_unseen_path': '../../data/way_splits/valUnseen_data.json',\n",
    "    'mesh2meters': '../../data/floorplans/pix2meshDistance.json',\n",
    "    'image_dir': '../../data/floorplans/',\n",
    "\n",
    "    # Hyper Parameters\n",
    "    'max_floors': 5,\n",
    "\n",
    "    # Image Parameters \n",
    "    'image_size': [3, 224, 224],\n",
    "    'original_image_size': [3, 700, 1200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7f3b42ae53a0>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utils:\n",
    "    class DotDict(dict):\n",
    "        \"\"\"\n",
    "        a dictionary that supports dot notation \n",
    "        as well as dictionary access notation \n",
    "        usage: d = DotDict() or d = DotDict({'val1':'first'})\n",
    "        set attributes: d.val2 = 'second' or d['val2'] = 'second'\n",
    "        get attributes: d.val2 or d['val2']\n",
    "        \"\"\"\n",
    "        __getattr__ = dict.__getitem__\n",
    "        __setattr__ = dict.__setitem__\n",
    "        __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'valSeen'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['valid_seen_path'].split('/')[-1][:-5].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 700)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('/home/saaket/embodiedAI/led_clip/data/floorplans/floor_0/1LXtFkjw3qL_0.png')\n",
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset \n",
    "\n",
    "class LEDDataset(Dataset):\n",
    "    def __init__(self, data_path, image_dir, config):\n",
    "\n",
    "        # Gather train_data from {train/val/test}_data.json\n",
    "        self.data_path = data_path \n",
    "        self.data_file = open(self.data_path)\n",
    "        self.data = json.load(self.data_file)\n",
    "\n",
    "        # Extract the mode (train, valSeen, valUnseen) from the data_path \n",
    "        self.mode = self.data_path.split('/')[-1][:-5].split('_')[0]\n",
    "\n",
    "        # Store access to floorplans directory \n",
    "        self.image_dir = image_dir \n",
    "\n",
    "        # Save the global config \n",
    "        self.config = config \n",
    "\n",
    "        # mesh2meters\n",
    "        self.mesh2meters_path = self.config['mesh2meters']\n",
    "        self.mesh2meters_file = open(self.mesh2meters_path)\n",
    "        self.mesh2meters = json.loads(self.mesh2meters_file)\n",
    "\n",
    "        # transform required for CLIP \n",
    "        def convert_image_to_rgb(image):\n",
    "            return image.convert(\"RGB\")\n",
    "\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(size=224, interpolation='bicubic', max_size=None, antialias=None),\n",
    "            transforms.CenterCrop(size=(224, 244)),\n",
    "            convert_image_to_rgb,\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def gather_all_floors(self, index):\n",
    "        all_maps = torch.zeros(\n",
    "            self.config.max_floors,\n",
    "            self.config.image_size[0],\n",
    "            self.config.image_size[1],\n",
    "            self.config.image_size[2],\n",
    "        )\n",
    "        all_conversions = torch.zeros(self.config.max_floors, 1)\n",
    "        scan_name = self.data[index]['scan_name']\n",
    "        floors = self.mesh2meters[scan_name].keys()\n",
    "        for enum, floor in enumerate(floors):\n",
    "            img = Image.open(f'{self.image_dir}floor_{floor}/{scan_name}_{floor}.png').convert('RGB')\n",
    "            img.size()\n",
    "            if \"train\" in self.mode:\n",
    "                all_maps[enum, :, :, :] = self.preprocess(img)[:3, :, :]\n",
    "            else:\n",
    "                all_maps[enum, :, :, :] = self.preprocess(img)[:3, :, :]\n",
    "            all_conversions[enum, :] = self.mesh2meters[scan_name][floor][\"threeMeterRadius\"] / 3.0\n",
    "        return all_maps, all_conversions\n",
    "    \n",
    "    def scale_location(self, location):\n",
    "        return {\n",
    "            'x': \n",
    "        }\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "\n",
    "             "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1915fb457f177848a436c53e0d85f261306c0429b7a27e35e26917a207fd56ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graphled')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
