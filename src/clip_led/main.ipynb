{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import json \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image \n",
    "import clip \n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import sys \n",
    "sys.path.append('../..')\n",
    "import src.utils as utils\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Data Paths\n",
    "    'train_path' : '../../data/way_splits/train_data.json',\n",
    "    'valid_seen_path' : '../../data/way_splits/valSeen_data.json',\n",
    "    'valid_unseen_path': '../../data/way_splits/valUnseen_data.json',\n",
    "    'mesh2meters': '../../data/floorplans/pix2meshDistance.json',\n",
    "    'image_dir': '../../data/floorplans/',\n",
    "\n",
    "    'device': 'cpu',\n",
    "\n",
    "    # Hyper Parameters\n",
    "    'max_floors': 5,\n",
    "\n",
    "    # Image Parameters \n",
    "    'image_size': [3, 224, 224],\n",
    "    # 'image_size': [3, 700, 1200],\n",
    "    'original_image_size': [3, 700, 1200],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset \n",
    "\n",
    "class LEDDataset(Dataset):\n",
    "    def __init__(self, data_path, image_dir, config):\n",
    "\n",
    "        # Gather train_data from {train/val/test}_data.json\n",
    "        self.data_path = data_path \n",
    "        self.data_file = open(self.data_path)\n",
    "        self.data = json.load(self.data_file)\n",
    "\n",
    "        # Extract the mode (train, valSeen, valUnseen) from the data_path \n",
    "        self.mode = self.data_path.split('/')[-1][:-5].split('_')[0]\n",
    "\n",
    "        # Store access to floorplans directory \n",
    "        self.image_dir = image_dir \n",
    "\n",
    "        # Save the global config \n",
    "        self.config = config \n",
    "\n",
    "        # mesh2meters\n",
    "        self.mesh2meters_path = self.config['mesh2meters']\n",
    "        self.mesh2meters_file = open(self.mesh2meters_path)\n",
    "        self.mesh2meters = json.load(self.mesh2meters_file)\n",
    "\n",
    "        # transform required for CLIP \n",
    "        def convert_image_to_rgb(image):\n",
    "            return image.convert(\"RGB\")\n",
    "\n",
    "        self.preprocess = transforms.Compose([\n",
    "            # transforms.Resize(size=(224, 224), interpolation=transforms.InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "            # transforms.CenterCrop(size=(224, 244)),\n",
    "            convert_image_to_rgb,\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "        self.preprocess_visualize = transforms.Compose([\n",
    "            # transforms.Resize(size=(224, 224), interpolation=transforms.InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "            # transforms.CenterCrop(size=(224, 244)),\n",
    "            convert_image_to_rgb,\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def gather_all_floors(self, index):\n",
    "        all_maps = torch.zeros(\n",
    "            self.config['max_floors'],\n",
    "            self.config[\"image_size\"][0],\n",
    "            self.config[\"image_size\"][1],\n",
    "            self.config[\"image_size\"][2],\n",
    "        )\n",
    "        all_conversions = torch.zeros(self.config[\"max_floors\"], 1)\n",
    "        scan_name = self.data[index]['scanName']\n",
    "        floors = self.mesh2meters[scan_name].keys()\n",
    "        for enum, floor in enumerate(floors):\n",
    "            img = Image.open(f'{self.image_dir}floor_{floor}/{scan_name}_{floor}.png').convert('RGB')\n",
    "            if \"train\" in self.mode:\n",
    "                temp = self.preprocess(img)\n",
    "                all_maps[enum, :, :, :] = self.preprocess(img)[:3, :, :]\n",
    "            else:\n",
    "                all_maps[enum, :, :, :] = self.preprocess(img)[:3, :, :]\n",
    "            all_conversions[enum, :] = self.mesh2meters[scan_name][floor][\"threeMeterRadius\"] / 3.0\n",
    "        return all_maps, all_conversions\n",
    "\n",
    "    def gather_correct_floor(self, index):\n",
    "        scan_name = self.data[index]['scanName']\n",
    "        x, y, floor = self.scale_location(index)\n",
    "        img = Image.open(f'{self.image_dir}floor_{floor}/{scan_name}_{floor}.png').convert('RGB')\n",
    "        \n",
    "        map = self.preprocess(img)\n",
    "        conversion = torch.tensor(self.mesh2meters[scan_name][str(floor)][\"threeMeterRadius\"] / 3.0).float()\n",
    "\n",
    "        return map, conversion\n",
    "\n",
    "    def scale_location(self, index):\n",
    "        if \"test\" in self.mode:\n",
    "            return [0, 0, 0]\n",
    "\n",
    "        floor = self.data[index]['finalLocation'][\"floor\"]\n",
    "        x, y = self.data[index]['finalLocation'][\"pixel_coord\"]    \n",
    "\n",
    "        return [int(x * (self.config['image_size'][2]/self.config['original_image_size'][2])), int(x * (self.config['image_size'][1]/self.config['original_image_size'][1])), floor] \n",
    "    \n",
    "\n",
    "    def create_target(self, index, location, mesh_conversion):\n",
    "        gaussian_target = np.zeros(\n",
    "            (self.args.max_floors, self.output_image_size[1], self.output_image_size[2])\n",
    "        )\n",
    "        gaussian_target[int(self.levels[index]), location[0], location[1]] = 1\n",
    "        gaussian_target[int(self.levels[index]), :, :] = gaussian_filter(\n",
    "            gaussian_target[int(self.levels[index]), :, :],\n",
    "            sigma=(mesh_conversion/60),\n",
    "        )\n",
    "        gaussian_target[int(self.levels[index]), :, :] = (\n",
    "            gaussian_target[int(self.levels[index]), :, :]\n",
    "            / gaussian_target[int(self.levels[index]), :, :].sum()\n",
    "        )\n",
    "        gaussian_target = torch.tensor(gaussian_target)\n",
    "        return gaussian_target\n",
    "        \n",
    "    def join_dialog(self, index):\n",
    "        dialogArray = self.data[index]['dialogArray']\n",
    "        return \" \".join(dialogArray)\n",
    "\n",
    "    def visualize_data(self, index):\n",
    "        all_maps = np.zeros((\n",
    "            self.config['max_floors'],\n",
    "            self.config[\"image_size\"][1],\n",
    "            self.config[\"image_size\"][2],\n",
    "            self.config[\"image_size\"][0],\n",
    "        )\n",
    "        )\n",
    "        scan_name = self.data[index]['scanName']\n",
    "        floors = self.mesh2meters[scan_name].keys()\n",
    "        images = []\n",
    "        for enum, floor in enumerate(floors):\n",
    "            img = Image.open(f'{self.image_dir}floor_{floor}/{scan_name}_{floor}.png').convert('RGB')\n",
    "            all_maps[enum] = torch.permute(self.preprocess_visualize(img)[:3, :, :], (1, 2, 0)).cpu().numpy()\n",
    "\n",
    "        # create figure\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        \n",
    "        # setting values to rows and column variables\n",
    "        rows = 3\n",
    "        columns = 2\n",
    "        # Adds a subplot at the 1st position\n",
    "        fig.add_subplot(rows, columns, 1)\n",
    "        \n",
    "        # showing image\n",
    "        plt.imshow(all_maps[0])\n",
    "        plt.Circle((100, 100), 50, color='k')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"First\")\n",
    "        \n",
    "        # Adds a subplot at the 2nd position\n",
    "        fig.add_subplot(rows, columns, 2)\n",
    "        \n",
    "        # showing image\n",
    "        plt.imshow(all_maps[1])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Second\")\n",
    "        \n",
    "        # Adds a subplot at the 3rd position\n",
    "        fig.add_subplot(rows, columns, 3)\n",
    "        \n",
    "        # showing image\n",
    "        plt.imshow(all_maps[2])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Third\")\n",
    "        \n",
    "        # Adds a subplot at the 4th position\n",
    "        fig.add_subplot(rows, columns, 4)\n",
    "        \n",
    "        # showing image\n",
    "        plt.imshow(all_maps[3])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Fourth\")\n",
    "\n",
    "        # Adds a subplot at the 4th position\n",
    "        fig.add_subplot(rows, columns, 5)\n",
    "        \n",
    "        # showing image\n",
    "        plt.imshow(all_maps[4])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Fourth\")\n",
    "\n",
    "    def visualize_target(self, index):\n",
    "        all_maps = np.zeros((\n",
    "            self.config['max_floors'],\n",
    "            self.config[\"image_size\"][1],\n",
    "            self.config[\"image_size\"][2],\n",
    "            self.config[\"image_size\"][0],\n",
    "        )\n",
    "        )\n",
    "        scan_name = self.data[index]['scanName']\n",
    "        floors = self.mesh2meters[scan_name].keys()\n",
    "        images = []\n",
    "        for enum, floor in enumerate(floors):\n",
    "            img = Image.open(f'{self.image_dir}floor_{floor}/{scan_name}_{floor}.png').convert('RGB')\n",
    "            all_maps[enum] = torch.permute(self.preprocess_visualize(img), (1, 2, 0)).cpu().numpy()\n",
    "        \n",
    "        x, y, floor = self.scale_location(index)\n",
    "        tensor  = all_maps[floor]       \n",
    "        fig,ax = plt.subplots(1)\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "        # Show the image\n",
    "        ax.imshow(tensor)\n",
    "        \n",
    "        circ = plt.Circle((x,y),10, color='red')\n",
    "        ax.add_patch(circ)\n",
    "        print(self.join_dialog(index))\n",
    "        # Show the image\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target_x, target_y, target_floor = self.scale_location(index)\n",
    "        # maps, conversions = self.gather_correct_floor(index)\n",
    "        maps, conversions = self.gather_all_floors(index)\n",
    "        dialog = clip.tokenize(self.join_dialog(index), truncate=True)\n",
    "        target = self.create_target(index, )\n",
    "\n",
    "        return {\n",
    "            'target_x': target_x,\n",
    "            'target_y': target_y,\n",
    "            'targets': torch.tensor([target_x, target_y]).float(),\n",
    "            'target_floors': torch.tensor(int(target_floor)).float(),\n",
    "            'maps': maps,\n",
    "            'conversions': conversions,\n",
    "            'dialogs': dialog\n",
    "        }\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1915fb457f177848a436c53e0d85f261306c0429b7a27e35e26917a207fd56ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graphled')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
