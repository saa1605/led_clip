{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/graphled/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import json \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image \n",
    "import clip \n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model ,p = clip.load('RN50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config = {\n",
    "    # Data Paths\n",
    "    'train_path' : '../../data/way_splits/train_data.json',\n",
    "    'valid_seen_path' : '../../data/way_splits/valSeen_data.json',\n",
    "    'valid_unseen_path': '../../data/way_splits/valUnseen_data.json',\n",
    "    'mesh2meters': '../../data/floorplans/pix2meshDistance.json',\n",
    "    'image_dir': '../../data/floorplans/',\n",
    "\n",
    "    'device': 'cpu',\n",
    "\n",
    "    # Hyper Parameters\n",
    "    'max_floors': 5,\n",
    "\n",
    "    # Image Parameters \n",
    "    'image_size': [3, 224, 224],\n",
    "    'original_image_size': [3, 700, 1200],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utils:\n",
    "    class DotDict(dict):\n",
    "        \"\"\"\n",
    "        a dictionary that supports dot notation \n",
    "        as well as dictionary access notation \n",
    "        usage: d = DotDict() or d = DotDict({'val1':'first'})\n",
    "        set attributes: d.val2 = 'second' or d['val2'] = 'second'\n",
    "        get attributes: d.val2 or d['val2']\n",
    "        \"\"\"\n",
    "        __getattr__ = dict.__getitem__\n",
    "        __setattr__ = dict.__setitem__\n",
    "        __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset \n",
    "\n",
    "class LEDDataset(Dataset):\n",
    "    def __init__(self, data_path, image_dir, config):\n",
    "\n",
    "        # Gather train_data from {train/val/test}_data.json\n",
    "        self.data_path = data_path \n",
    "        self.data_file = open(self.data_path)\n",
    "        self.data = json.load(self.data_file)\n",
    "\n",
    "        # Extract the mode (train, valSeen, valUnseen) from the data_path \n",
    "        self.mode = self.data_path.split('/')[-1][:-5].split('_')[0]\n",
    "\n",
    "        # Store access to floorplans directory \n",
    "        self.image_dir = image_dir \n",
    "\n",
    "        # Save the global config \n",
    "        self.config = config \n",
    "\n",
    "        # mesh2meters\n",
    "        self.mesh2meters_path = self.config['mesh2meters']\n",
    "        self.mesh2meters_file = open(self.mesh2meters_path)\n",
    "        self.mesh2meters = json.load(self.mesh2meters_file)\n",
    "\n",
    "        # transform required for CLIP \n",
    "        def convert_image_to_rgb(image):\n",
    "            return image.convert(\"RGB\")\n",
    "\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(size=(224, 224), interpolation=transforms.InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "            # transforms.CenterCrop(size=(224, 244)),\n",
    "            convert_image_to_rgb,\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def gather_all_floors(self, index):\n",
    "        all_maps = torch.zeros(\n",
    "            self.config['max_floors'],\n",
    "            self.config[\"image_size\"][0],\n",
    "            self.config[\"image_size\"][1],\n",
    "            self.config[\"image_size\"][2],\n",
    "        )\n",
    "        all_conversions = torch.zeros(self.config[\"max_floors\"], 1)\n",
    "        scan_name = self.data[index]['scanName']\n",
    "        floors = self.mesh2meters[scan_name].keys()\n",
    "        for enum, floor in enumerate(floors):\n",
    "            img = Image.open(f'{self.image_dir}floor_{floor}/{scan_name}_{floor}.png').convert('RGB')\n",
    "            if \"train\" in self.mode:\n",
    "                temp = self.preprocess(img)\n",
    "                all_maps[enum, :, :, :] = self.preprocess(img)[:3, :, :]\n",
    "            else:\n",
    "                all_maps[enum, :, :, :] = self.preprocess(img)[:3, :, :]\n",
    "            all_conversions[enum, :] = self.mesh2meters[scan_name][floor][\"threeMeterRadius\"] / 3.0\n",
    "        return all_maps, all_conversions\n",
    "\n",
    "    def scale_location(self, index):\n",
    "        if \"test\" in self.mode:\n",
    "            return [0, 0, 0]\n",
    "\n",
    "        floor = self.data[index]['finalLocation'][\"floor\"]\n",
    "        x, y = self.data[index]['finalLocation'][\"pixel_coord\"]    \n",
    "\n",
    "        return [int(x * (self.config['image_size'][2]/self.config['original_image_size'][2])), int(x * (self.config['image_size'][1]/self.config['original_image_size'][1])), floor] \n",
    "        \n",
    "    def join_dialog(self, index):\n",
    "        dialogArray = self.data[index]['dialogArray']\n",
    "        return \" \".join(dialogArray)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target_x, target_y, target_floor = self.scale_location(index)\n",
    "        maps, conversions = self.gather_all_floors(index)\n",
    "        dialog = clip.tokenize(self.join_dialog(index))\n",
    "\n",
    "        return {\n",
    "            'target_x': target_x,\n",
    "            'target_y': target_y,\n",
    "            'target': torch.tensor([target_x, target_y]).float(),\n",
    "            'target_floor': torch.tensor(int(target_floor)).float(),\n",
    "            'maps': maps,\n",
    "            'conversions': conversions,\n",
    "            'dialog': dialog\n",
    "        }\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LEDDataset(config['train_path'], config['image_dir'], config)\n",
    "valid_seen_dataset = LEDDataset(config['valid_seen_path'], config['image_dir'], config)\n",
    "valid_unseen_dataset = LEDDataset(config['valid_unseen_path'], config['image_dir'], config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPLocator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CLIPLocator, self).__init__()\n",
    "        self.CLIP, _ = clip.load(\"RN50\")\n",
    "        self.config = config\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(2048, 1024)\n",
    "        self.regressionLinear = nn.Linear(1024, 2)\n",
    "        self.floorClassificationLinear = nn.Linear(1024, 5)\n",
    "\n",
    "    def forward(self, map, dialog):\n",
    "        batch_size, num_maps, channels, height, width = map.size()\n",
    "        map = map.view(batch_size * num_maps, channels, height, width)\n",
    "        image_enc = self.CLIP.encode_image(map)\n",
    "        text_enc = self.CLIP.encode_text(dialog)\n",
    "        text_enc = torch.repeat_interleave(text_enc, num_maps, 0)\n",
    "        enc = torch.cat((image_enc, text_enc), dim=-1)\n",
    "        dropoutOut = self.dropout(enc)\n",
    "        linear1Out = F.relu(self.linear1(dropoutOut))\n",
    "        regressionOut = self.regressionLinear(linear1Out)\n",
    "        floorclassificationOut = self.floorClassificationLinear(linear1Out)\n",
    "\n",
    "        return regressionOut, floorclassificationOut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, scheduler, device, config):\n",
    "    # Training \n",
    "    for idx, data in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "        maps = data['maps'].to(device)\n",
    "        conversations = data['conversations']\n",
    "        \n",
    "        regressionPreds, classificationPreds = model()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = CLIPLocator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Load Data \n",
    "\n",
    "\n",
    "    # Define model, loss and optimizer \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(loc.parameters())\n",
    "    scheduler = torch.optim.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    # Run train and validation loop \n",
    "\n",
    "\n",
    "    # Run Test \n",
    "\n",
    "\n",
    "    # Save results "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1915fb457f177848a436c53e0d85f261306c0429b7a27e35e26917a207fd56ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graphled')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
