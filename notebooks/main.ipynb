{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "import clip\n",
    "from PIL import Image\n",
    "import json \n",
    "from torchvision import models, transforms\n",
    "import clip\n",
    "import src.clip_lingunet.fusion as fusion\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 77])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"some words\", \"something else\", \"this is insane\"]\n",
    "tok = clip.tokenize(a)\n",
    "tok.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 77])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_all_maps = torch.repeat_interleave(tok, 5, dim=0)\n",
    "\n",
    "tok_all_maps.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   836,  2709,  ...,     0,     0,     0],\n",
       "        [49406,   836,  2709,  ...,     0,     0,     0],\n",
       "        [49406,   836,  2709,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [49406,   589,   533,  ...,     0,     0,     0],\n",
       "        [49406,   589,   533,  ...,     0,     0,     0],\n",
       "        [49406,   589,   533,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_all_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityBlock(nn.Module):\n",
    "    def __init__(self, in_planes, filters, kernel_size, stride=1, final_relu=True, batchnorm=True):\n",
    "        super(IdentityBlock, self).__init__()\n",
    "        self.final_relu = final_relu\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        filters1, filters2, filters3 = filters\n",
    "        self.conv1 = nn.Conv2d(in_planes, filters1, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(filters1) if self.batchnorm else nn.Identity()\n",
    "        self.conv2 = nn.Conv2d(filters1, filters2, kernel_size=kernel_size, dilation=1,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(filters2) if self.batchnorm else nn.Identity()\n",
    "        self.conv3 = nn.Conv2d(filters2, filters3, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(filters3) if self.batchnorm else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += x\n",
    "        if self.final_relu:\n",
    "            out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_planes, filters, kernel_size, stride=1, final_relu=True, batchnorm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.final_relu = final_relu\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        filters1, filters2, filters3 = filters\n",
    "        self.conv1 = nn.Conv2d(in_planes, filters1, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(filters1) if self.batchnorm else nn.Identity()\n",
    "        self.conv2 = nn.Conv2d(filters1, filters2, kernel_size=kernel_size, dilation=1,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(filters2) if self.batchnorm else nn.Identity()\n",
    "        self.conv3 = nn.Conv2d(filters2, filters3, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(filters3) if self.batchnorm else nn.Identity()\n",
    "\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, filters3,\n",
    "                      kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(filters3) if self.batchnorm else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        if self.final_relu:\n",
    "            out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),                                     # (Mohit): argh... forgot to remove this batchnorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),                                     # (Mohit): argh... forgot to remove this batchnorm\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'floorplans': '../data/floorplans',\n",
    "    'dialogs': '../data/way_splits'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"RN50\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = preprocess(Image.open(paths['floorplans'] + '/floor_0/Uxmj2M2itWa_0.png'))[:3].unsqueeze(0).to(device)\n",
    "\n",
    "dialogArray = [\n",
    "            \"What kind of room are you in?\", \n",
    "            \"I am standing just outside a bathroom near a purple chair.\", \n",
    "            \"Is there a red bed by you?\", \n",
    "            \"yes, the purple chair is between myself and the red bed.\"\n",
    "        ]\n",
    "text = ' '.join(dialogArray)\n",
    "# tokens = clip.tokenize(text).to(device)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7f0f18f3f280>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_all_floors(preprocess):\n",
    "    image_size = [\n",
    "            3,\n",
    "            224,\n",
    "            224,\n",
    "        ]\n",
    "\n",
    "    # preprocess = transforms.Compose(\n",
    "    #         [\n",
    "    #             transforms.ToTensor(),\n",
    "    #             transforms.Normalize(\n",
    "    #                 mean=[0.485, 0.456, 0.406, 0.555],\n",
    "    #                 std=[0.229, 0.224, 0.225, 0.222],\n",
    "    #             ),\n",
    "    #         ]\n",
    "    #     )\n",
    "    all_maps = torch.zeros(\n",
    "            5,\n",
    "            image_size[0],\n",
    "            image_size[1],\n",
    "            image_size[2],\n",
    "        )\n",
    "    all_conversions = torch.zeros(5, 1)\n",
    "    sn = 'Uxmj2M2itWa'\n",
    "    mesh2meters = json.load(open(paths['floorplans'] + \"/pix2meshDistance.json\"))\n",
    "    floors = mesh2meters[sn].keys()\n",
    "    for enum, f in enumerate(floors):\n",
    "        img = Image.open(\n",
    "            \"{}/floor_{}/{}_{}.png\".format(paths['floorplans'], f, sn, f)\n",
    "        )\n",
    "        img = img.resize((image_size[2], image_size[1]))\n",
    "\n",
    "        all_maps[enum, :, :, :] = preprocess(img)[:3, :, :]\n",
    "        all_conversions[enum, :] = mesh2meters[sn][f][\"threeMeterRadius\"] / 3.0\n",
    "    return all_maps, all_conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps, convs = gather_all_floors(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3, 224, 224])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps = maps.unsqueeze(0); maps.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5, 3, 224, 224])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps = torch.repeat_interleave(maps, 6, dim=0); maps.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = maps.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPLingUNet(nn.Module):\n",
    "    \"\"\" CLIP RN50 with U-Net skip connections \"\"\"\n",
    "    def __init__(self, args):\n",
    "        super(CLIPLingUNet, self).__init__()\n",
    "        self.args = args \n",
    "        # self.output_dim = self.args.output_dim\n",
    "        self.output_dim = 1\n",
    "        self.input_dim = 2048  # penultimate layer channel-size of CLIP-RN50\n",
    "        self.device = self.args.device \n",
    "        self.batchnorm = True\n",
    "        self.lang_fusion_type = 'mult'\n",
    "        self.bilinear = True\n",
    "        self.up_factor = 2 if self.bilinear else 1\n",
    "        self.clip_rn50, self.preprocess = clip.load(\"RN50\", device=self.args.device)\n",
    "\n",
    "        self._build_decoder()\n",
    "\n",
    "\n",
    "    def _build_decoder(self):\n",
    "        # language\n",
    "        self.lang_fuser1 = fusion.names[self.lang_fusion_type](input_dim=self.input_dim // 2)\n",
    "        self.lang_fuser2 = fusion.names[self.lang_fusion_type](input_dim=self.input_dim // 4)\n",
    "        self.lang_fuser3 = fusion.names[self.lang_fusion_type](input_dim=self.input_dim // 8)\n",
    "\n",
    "        # CLIP encoder output -> 1024\n",
    "        self.proj_input_dim = 512 if 'word' in self.lang_fusion_type else 1024\n",
    "        self.lang_proj1 = nn.Linear(self.proj_input_dim, 1024)\n",
    "        self.lang_proj2 = nn.Linear(self.proj_input_dim, 512)\n",
    "        self.lang_proj3 = nn.Linear(self.proj_input_dim, 256)\n",
    "\n",
    "        # vision\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.up1 = Up(2048, 1024 // self.up_factor, self.bilinear)\n",
    "\n",
    "        self.up2 = Up(1024, 512 // self.up_factor, self.bilinear)\n",
    "\n",
    "        self.up3 = Up(512, 256 // self.up_factor, self.bilinear)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ConvBlock(128, [64, 64, 64], kernel_size=3, stride=1, batchnorm=self.batchnorm),\n",
    "            IdentityBlock(64, [64, 64, 64], kernel_size=3, stride=1, batchnorm=self.batchnorm),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConvBlock(64, [32, 32, 32], kernel_size=3, stride=1, batchnorm=self.batchnorm),\n",
    "            IdentityBlock(32, [32, 32, 32], kernel_size=3, stride=1, batchnorm=self.batchnorm),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ConvBlock(32, [16, 16, 16], kernel_size=3, stride=1, batchnorm=self.batchnorm),\n",
    "            IdentityBlock(16, [16, 16, 16], kernel_size=3, stride=1, batchnorm=self.batchnorm),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, self.output_dim, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def encode_image(self, img):\n",
    "        with torch.no_grad():\n",
    "            img_encoding, img_im = self.clip_rn50.visual.prepool_im(img)\n",
    "        return img_encoding, img_im\n",
    "\n",
    "    def encode_text(self, x):\n",
    "        with torch.no_grad():\n",
    "            tokens = clip.tokenize([x]).to(self.device)\n",
    "            \n",
    "            text_feat = self.clip_rn50.encode_text(tokens)\n",
    "\n",
    "        text_mask = torch.where(tokens==0, tokens, 1)  # [1, max_token_len]\n",
    "        return text_feat, text_mask\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, l):\n",
    "        # x = self.preprocess(x)\n",
    "\n",
    "        in_type = x.dtype\n",
    "        in_shape = x.shape\n",
    "        x = x[:,:3]  # select RGB\n",
    "        x, im = self.encode_image(x)\n",
    "        x = x.to(in_type)\n",
    "\n",
    "        # encode text\n",
    "        l_enc, l_mask = self.encode_text(l)\n",
    "        l_input = l_enc\n",
    "        l_input = l_input.to(dtype=x.dtype)\n",
    "\n",
    "        # # encode image\n",
    "        assert x.shape[1] == self.input_dim\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.lang_fuser1(x, l_input, x2_mask=l_mask, x2_proj=self.lang_proj1)\n",
    "        x = self.up1(x, im[-2])\n",
    "\n",
    "        x = self.lang_fuser2(x, l_input, x2_mask=l_mask, x2_proj=self.lang_proj2)\n",
    "        x = self.up2(x, im[-3])\n",
    "\n",
    "        x = self.lang_fuser3(x, l_input, x2_mask=l_mask, x2_proj=self.lang_proj3)\n",
    "        x = self.up3(x, im[-4])\n",
    "\n",
    "        for layer in [self.layer1, self.layer2, self.layer3, self.conv2]:\n",
    "            x = layer(x)\n",
    "\n",
    "        # x = F.interpolate(x, size=(780, 455), mode='bilinear')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = CLIPLingUNet(args)\n",
    "cli.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 3, 224, 224])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps = maps.view(6*5, 3, 224, 224); maps.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = maps.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cli(maps, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 1, 448, 448])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 448, 448])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.squeeze(1); out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5, 448, 448])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.view(6, 5, out.size()[-2], out.size()[-1])\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = F.log_softmax(out.view(6, -1), 1).view(6, 5, 448, 448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5, 448, 448])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.to(device)\n",
    "modules = list(resnet.children())[:-4]\n",
    "resnetPrePool = torch.nn.Sequential(*modules)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = resnetPrePool(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 28, 28])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img[:,:3]\n",
    "img_encoding, img_im = model.visual.prepool_im(img)\n",
    "\n",
    "upsample = torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 28, 28])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_im[-3].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 7, 7])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_encoding.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = model.encode_image(img)\n",
    "text_features = model.encode_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1024]), torch.Size([3, 1024]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.size(), text_features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1915fb457f177848a436c53e0d85f261306c0429b7a27e35e26917a207fd56ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graphled')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
