{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/graphled/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import json \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import sys \n",
    "sys.path.append('../..')\n",
    "import src.utils as utils\n",
    "import src.clip as clip \n",
    "import yaml\n",
    "import math \n",
    "from tqdm import tqdm  \n",
    "from src.clip_led.dataset import LEDDataset\n",
    "\n",
    "import src.fusion as fusion\n",
    "from src.blocks import Up, ConvBlock, IdentityBlock\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Data Paths\n",
    "    'train_path' : '../../data/way_splits/train_data.json',\n",
    "    'valid_seen_path' : '../../data/way_splits/valSeen_data.json',\n",
    "    'valid_unseen_path': '../../data/way_splits/valUnseen_data.json',\n",
    "    'mesh2meters': '../../data/floorplans/pix2meshDistance.json',\n",
    "    'image_dir': '../../data/floorplans/',\n",
    "    'geodistance_file': '../../data/geodistance_nodes.json',\n",
    "\n",
    "    'device': 'cpu',\n",
    "\n",
    "    # Hyper Parameters\n",
    "    'max_floors': 5,\n",
    "\n",
    "    # Image Parameters\n",
    "    'image_size': [3, 448, 448],\n",
    "    # 'image_size': [3, 700, 1200],\n",
    "    'original_image_size': [3, 700, 1200],\n",
    "    'cropped_image_size': [3, 700, 800],\n",
    "    'scaled_image_size': [3, 448, 448],\n",
    "\n",
    "\n",
    "    'crop_translate_x': 200,\n",
    "    'crop_translate_y': 0,\n",
    "    'resize_scale_x': 448/800,\n",
    "    'resize_scale_y': 448/700,\n",
    "    'conversion_scale': 448/800,\n",
    "\n",
    "\n",
    "    'lang_fusion_type': 'mult',\n",
    "    'num_post_clip_channels': 2048, \n",
    "    'bilinear': True,\n",
    "    'batch_norm': True, \n",
    "    'num_output_channels': 1,\n",
    "\n",
    "    'lr': 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LEDDataset(config['valid_seen_path'], config['image_dir'], config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "led_clip = LEDModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/graphled/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters \n",
    "\n",
    "loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "optimizer = torch.optim.AdamW(led_clip.parameters(), lr=config['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "scaler = torch.cuda.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def snap_to_grid(geodistance_nodes, node2pix, sn, pred_coord, conversion, level):\n",
    "    min_dist = math.inf\n",
    "    best_node = \"\"\n",
    "    for node in node2pix[sn].keys():\n",
    "        if node2pix[sn][node][2] != int(level) or node not in geodistance_nodes:\n",
    "            continue\n",
    "        target_coord = [node2pix[sn][node][0][1], node2pix[sn][node][0][0]]\n",
    "        dist = np.sqrt(\n",
    "            (target_coord[0] - pred_coord[0]) ** 2\n",
    "            + (target_coord[1] - pred_coord[1]) ** 2\n",
    "        ) / (conversion)\n",
    "        if dist.item() < min_dist:\n",
    "            best_node = node\n",
    "            min_dist = dist.item()\n",
    "    return best_node\n",
    "\n",
    "\n",
    "def distance_from_pixels(config, preds, mesh_conversions, scan_names, true_viewpoints, episode_ids, mode):\n",
    "    \"\"\"Calculate distances between model predictions and targets within a batch.\n",
    "    Takes the propablity map over the pixels and returns the geodesic distance\"\"\"\n",
    "    node2pix = json.load(open(config['image_dir'] + \"allScans_Node2pix.json\"))\n",
    "    geodistance_nodes = json.load(open(config['geodistance_file']))\n",
    "    distances, episode_predictions = [], []\n",
    "    for pred, conversion, sn, tv, id in zip(\n",
    "        preds, mesh_conversions, scan_names, true_viewpoints, episode_ids\n",
    "    ):\n",
    "\n",
    "        total_floors = len(set([v[2] for k, v in node2pix[sn].items()]))\n",
    "        pred = nn.functional.interpolate(\n",
    "            pred.unsqueeze(1), (700, 1200), mode=\"bilinear\"\n",
    "        ).squeeze(1)[:total_floors]\n",
    "        pred_coord = np.unravel_index(pred.argmax(), pred.size())\n",
    "        convers = conversion.view(config['max_floors'], 1, 1)[pred_coord[0].item()]\n",
    "        pred_viewpoint = snap_to_grid(\n",
    "            geodistance_nodes[sn],\n",
    "            node2pix,\n",
    "            sn,\n",
    "            [pred_coord[1].item(), pred_coord[2].item()],\n",
    "            convers,\n",
    "            pred_coord[0].item(),\n",
    "        )\n",
    "        if mode != \"test\":\n",
    "            dist = geodistance_nodes[sn][tv][pred_viewpoint]\n",
    "            distances.append(dist)\n",
    "        episode_predictions.append([id, pred_viewpoint])\n",
    "    return distances, episode_predictions\n",
    "\n",
    "def accuracy(dists, threshold=3):\n",
    "    \"\"\"Calculating accuracy at 3 meters by default\"\"\"\n",
    "    return np.mean((torch.tensor(dists) <= threshold).int().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.478775426276762\n",
      "16.93066725972494\n",
      "torch.Size([2, 77])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    maps = data['maps']\n",
    "    target_maps = data['target_maps']\n",
    "    conversions = data['conversions']\n",
    "    dialogs = data['dialogs']\n",
    "    dialogs = dialogs.squeeze(1)\n",
    "\n",
    "    preds = led_clip(maps, dialogs)\n",
    "    break \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/graphled/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "le, ep = distance_from_pixels(config, preds, data['conversions'], data['scan_names'], data['true_viewpoints'], data['episode_ids'], train_dataset.mode )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in led_clip.parameters():\n",
    "    assert param.dtype == torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a': 0, 'b': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_a(a):\n",
    "    a['a'] = 6\n",
    "    a['b'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop \n",
    "\n",
    "\n",
    "def training_loop(train_loader, valid_seen_loader, valid_unseen_loader, epochs, model, loss_fn, optimizer, scaler, scheduler, config):\n",
    "\n",
    "    # Metrics \n",
    "    metrics = {\n",
    "        'train_loss': 0,\n",
    "        'valid_seen_loss': 0,\n",
    "        'valid_unseen_loss': 0,\n",
    "        'train_acc_5m': 0, \n",
    "        'train_acc_3m': 0, \n",
    "        'train_acc_0m': 0, \n",
    "        'valid_seen_acc_5m': 0, \n",
    "        'valid_seen_acc_3m': 0, \n",
    "        'valid_seen_acc_0m': 0, \n",
    "        'valid_unseen_acc_5m': 0,\n",
    "        'valid_unsseen_acc_3m': 0,\n",
    "        'valid_unsseen_acc_0m': 0,\n",
    "    }\n",
    "    \n",
    "    # Training \n",
    "    for e in range(epochs): \n",
    "\n",
    "        model.train()\n",
    "        train_metrics = train_model(model, loss_fn, optimizer, scaler, config)\n",
    "        \n",
    "        print(f'Train Loss: {train_metrics[\"loss\"]}')\n",
    "        print(f'Train Acc0m: {train_metrics[\"acc0m\"]}')\n",
    "        print(f'Train Acc3m: {train_metrics[\"acc3m\"]}')\n",
    "        print(f'Train Acc5m: {train_metrics[\"acc5m\"]}')\n",
    "        \n",
    "        assign_metrics(metrics, train_metrics, 'train')\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        valid_seen_metrics = eval_model(model, valid_seen_loader, loss_fn, config, 'valid_seen')\n",
    "\n",
    "        print(f'Valid Seen Loss: {valid_seen_metrics[\"loss\"]}')\n",
    "        print(f'Valid Seen Acc0m: {valid_seen_metrics[\"acc0m\"]}')\n",
    "        print(f'Valid Seen Acc3m: {valid_seen_metrics[\"acc3m\"]}')\n",
    "        print(f'Valid Seen Acc5m: {valid_seen_metrics[\"acc5m\"]}')\n",
    "\n",
    "        assign_metrics(metrics, valid_seen_metrics, 'valid_seen')\n",
    "\n",
    "        valid_unseen_metrics = eval_model(model, valid_seen_loader, loss_fn, config, 'valid_unseen')\n",
    "\n",
    "        print(f'Valid Unseen Loss: {valid_seen_metrics[\"loss\"]}')\n",
    "        print(f'Valid Unseen Acc0m: {valid_seen_metrics[\"acc0m\"]}')\n",
    "        print(f'Valid Unseen Acc3m: {valid_seen_metrics[\"acc3m\"]}')\n",
    "        print(f'Valid Unseen Acc5m: {valid_seen_metrics[\"acc5m\"]}')\n",
    "\n",
    "        assign_metrics(metrics, valid_unseen_metrics, 'valid_unseen')\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "        \n",
    "        scheduler.step(metrics['valid_unseen_loss'])\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1915fb457f177848a436c53e0d85f261306c0429b7a27e35e26917a207fd56ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graphled')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
